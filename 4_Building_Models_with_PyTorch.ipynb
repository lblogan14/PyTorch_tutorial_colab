{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4. Building Models with PyTorch.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPOdypJBdKhmSVCRj+N79NT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lblogan14/PyTorch_tutorial_colab/blob/main/4_Building_Models_with_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#`torch.nn.Module` and `torch.nn.Parameter`"
      ],
      "metadata": {
        "id": "C-8KxxaqBj5Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Except for `Parameter`, the classes we discuss here are all subclasses of `torch.nn.Module`. This is the PyTorch base class meant to encapsulate behaviors specific to PyTorch Models and their components.\n",
        "\n",
        "If a particular `Module` subclass has learning weights, these weights are expressed as instances of `torch.nn.Parameter`. The `Parameter` class is a subclass of `torch.Tensor`, with the special behavior that when they are assigned as attributes of a `Module`, they are added to the list of that module's parameters. These parameters may be accessed through the `parameters()` method on the `Module` class."
      ],
      "metadata": {
        "id": "LZHJ7VptBnuT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we created an instance of `Module` and ask it to report on its parameters:"
      ],
      "metadata": {
        "id": "6Mum2vXLCoy4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UjSjbARBbM7"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TinyModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TinyModel, self).__init__()\n",
        "\n",
        "        self.linear1 = torch.nn.Linear(100, 200)\n",
        "        self.activation = torch.nn.ReLU()\n",
        "        self.linear2 = torch.nn.Linear(200, 10)\n",
        "        self.softmax = torch.nn.Softmax()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.softmax(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "hYZa26MbCvIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tinymodel = TinyModel()\n",
        "\n",
        "print('The model:')\n",
        "print(tinymodel)\n",
        "print('\\n\\nJust one layer:')\n",
        "print(tinymodel.linear2)\n",
        "print('\\n\\nModel params:')\n",
        "for param in tinymodel.parameters():\n",
        "    print(param)\n",
        "\n",
        "print('\\n\\nLayer params:')\n",
        "for param in tinymodel.linear2.parameters():\n",
        "    print(param)"
      ],
      "metadata": {
        "id": "k4coK7GiDHxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This shows the fundamental structure of a PyTorch model: there is an `__init__()` method that defines the layers and other components of a model, and a `forward()` method where the computation gets done."
      ],
      "metadata": {
        "id": "feNAebVQD3vk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Common Layer Types"
      ],
      "metadata": {
        "id": "QN3mcEpTEIgd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Linear Layers\n",
        "most basic type - linear or fully connected layer.\n",
        "\n",
        "If a model has *m* inputs and *n* outputs, the weights will be an [m x n] matrix:"
      ],
      "metadata": {
        "id": "xD7DY8_zEJ5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lin = torch.nn.Linear(3,2)\n",
        "x = torch.rand(1,3)\n",
        "print('Input:')\n",
        "print(x)\n",
        "\n",
        "print('\\n\\nWeight and Bias parameters:')\n",
        "for param in lin.parameters():\n",
        "    print(param)\n",
        "\n",
        "y = lin(x)\n",
        "print('\\n\\nOutput:')\n",
        "print(y)"
      ],
      "metadata": {
        "id": "XDRMx7DeD04j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we checked the weights with `lin.weight`, it reported itself as a `Parameter`, and let us know that it is tracking gradients with autograd."
      ],
      "metadata": {
        "id": "zPLKG4MWFGve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lin.weight"
      ],
      "metadata": {
        "id": "ASXDYWTuE26Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lin.bias"
      ],
      "metadata": {
        "id": "iRShKsguFVDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Convolutional Layers\n",
        "Conv layers are built to handle data with a high degree of spatial correlation."
      ],
      "metadata": {
        "id": "_z_hS4AVFbZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.functional as F"
      ],
      "metadata": {
        "id": "A9NbaYdqFYPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LeNet(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "\n",
        "        # 1 input image channel (black&white), 6 output channels, 3x3 square convolution kernel\n",
        "        self.conv1 = torch.nn.Conv2d(1, 6, 5)\n",
        "        self.conv2 = torch.nn.Conv2d(6, 16, 3)\n",
        "\n",
        "        # an affine operation y = Wx + b\n",
        "        self.fc1 = torch.nn.Linear(16*6*6, 120) # 6*6 from image dimension\n",
        "        self.fc2 = torch.nn.Linear(120, 84)\n",
        "        self.fc3 = torch.nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Maxpooling over (2,2) window\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n",
        "        # If the size is a square we can only specify a single number\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:] # all dimensions except the batch dimension\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features"
      ],
      "metadata": {
        "id": "fbN90Hd_FmVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* LeNet5 takes in a 1x32x32 black & white image.\n",
        "* A Conv layer is like a window that scans over the image, looking for a pattern it recognizes.\n",
        "* The 3rd argument of `Conv2d` is the window or kernel size."
      ],
      "metadata": {
        "id": "YK5InZHQGzII"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output of a conv layer is an activation map - a spatial representation of the presence of features in the input tensor."
      ],
      "metadata": {
        "id": "wKbPNZCPHSdT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Recurrent Layers\n",
        "RNN are used for sequential data - anything from time-series measurements from a scientific instrucment to natural language sentences to DNA nucleotides. An RNN does this by maintaining a hidden state that acts as a sort of memory for what it has seen in the sequence so far.\n",
        "\n",
        "The internal structrue of an RNN layer - or its variants, the LSTM and GRU:"
      ],
      "metadata": {
        "id": "AJDsStk5HfOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMTagger(torch.nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "        super(LSTMTagger, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.word_embeddings = torch.nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states with dim of hidden_dim\n",
        "        self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim)\n",
        "\n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "        self.hidden2tag = torch.nn.Linear(hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
        "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores"
      ],
      "metadata": {
        "id": "1T2yEnrXH7H-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The constructor has four arguments:\n",
        "* `vocab_size` is the number of words in the input vocabulary. Each word is a one-hot vector in a `vocab_size`-dimensional space.\n",
        "* `tagset_size` is the number of tags in the output set.\n",
        "* `embedding_dim` is the size of the embedding space for the vocabulary. An embedding maps a vocabulary onto a low-dimensional space, where words with similar meanings are close together in the space.\n",
        "* `hidden_dim` is the size of the LSTM's memory\n",
        "\n",
        "The input will be a sentence with the words represented as indices of one-hot vectors. The embedding layer will then map these down to an `embedding_dim`-dimensional space. The LSTM takes this sequence of embeddings and iterates over it, fielding an output vector of length `hidden_dim`. The final linear layer acts as a classifier; applying `log_softmax()` to the output of the final layer converts the output into a normalized set of estimated probabilities that a given word maps to a given tag."
      ],
      "metadata": {
        "id": "ydtD-CU2Iv2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transformer\n",
        "PyTorch has a `Transformer` class that allows us to defin the overall parameters of a transformer model - the number of attention heads, the number of encoder & decoder layers, dropout and activation functions, etc.\n",
        "\n",
        "The `torch.nn.Transformer` class also has classes to encapsulate the individual components (`TransformerEncoder`, `TransformerDecoder`) and subcomponents (`TransformerEncoderLayer`, `TransformerDecoderLayer`)\n",
        "\n",
        "[Official documentation about Transformer class](https://pytorch.org/docs/stable/nn.html#transformer-layers)\n",
        "\n",
        "[Official Transformer Tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)"
      ],
      "metadata": {
        "id": "hkCBEvQmKPUv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Other Layers and Functions"
      ],
      "metadata": {
        "id": "WbtGEXBiLHG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Manipulation Layers\n",
        "There are layer types tha perform important functions in models but do not participate in the learning process."
      ],
      "metadata": {
        "id": "L09Wfk_ULJHp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Max pooling** reduces a tensor by combining cells, and assigning the max value of the input cells to the output cell. (similar to min pooling)"
      ],
      "metadata": {
        "id": "w08WMtJBOyfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_tensor = torch.rand(1, 6, 6)\n",
        "print(my_tensor)\n",
        "\n",
        "maxpool_layer = torch.nn.MaxPool2d(3)\n",
        "print(maxpool_layer(my_tensor))"
      ],
      "metadata": {
        "id": "C9lHgxnFO8ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normalization layers** re-center and normalize the output of one layer before feeding it to another. Centering and scaling the intermediate tensors has a number of beneficial effects, such as letting us use higher learning rates without exploding/vanishing gradients:"
      ],
      "metadata": {
        "id": "MUtKrgbTPGRu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_tensor = torch.rand(1,4,4) * 20 + 5\n",
        "print('My tensor:')\n",
        "print(my_tensor)\n",
        "\n",
        "print('\\nMean of my tensor:')\n",
        "print(my_tensor.mean())\n",
        "\n",
        "norm_layer = torch.nn.BatchNorm1d(4)\n",
        "normed_tensor = norm_layer(my_tensor)\n",
        "print('\\n\\nNormalized tensor:')\n",
        "print(normed_tensor)\n",
        "print('\\nMean of normalized tensor:')\n",
        "print(normed_tensor.mean())"
      ],
      "metadata": {
        "id": "zdh0ph7BPEZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The normalization layer is beneficial because many activation functions have their strongest gradients near 0, but sometimes suffer from vanishing or exploding gradients for inputs that drive them far away from zero. Keeping the data centered around the area of steepest gradient will tend to mean faster, better learning and higher feasible learning rates."
      ],
      "metadata": {
        "id": "V3cQ6oDJP2fK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dropout layers** are a tool for encouraging sparse representations in model - that is, pushing it to do inference with less data.\n",
        "\n",
        "Dropout layers work by randomly setting parts of the input tensor during training - they always turned off for inference. This forces the model to learn against this masked or reduced dataset:"
      ],
      "metadata": {
        "id": "9fRFlx9wQHSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_tensor = torch.rand(1, 4, 4)\n",
        "\n",
        "dropout = torch.nn.Dropout(p=0.4)\n",
        "print(dropout(my_tensor))\n",
        "print(dropout(my_tensor))"
      ],
      "metadata": {
        "id": "pq0Tl5VLPyEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Activation Functions\n",
        "Inserting non-linear activaiton functions between layers is what allows a deep learning model to simulate any function, rather than just linear ones.\n",
        "\n",
        "`torch.nn.Module` has objects encapsulating all of the major activation functions including ReLU and its many variants, Tanh, Hardtanh, sigmoid, and more."
      ],
      "metadata": {
        "id": "99d45HdfQkjH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loss Functions\n",
        "Loss functions tell us how far a model's prediction is from the correct answer."
      ],
      "metadata": {
        "id": "M6fPyqReQ-hm"
      }
    }
  ]
}