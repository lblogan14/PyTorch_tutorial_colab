{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3. Autograd.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNx3sCbAb+4bQ/VLdtFy4o5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lblogan14/PyTorch_tutorial_colab/blob/main/3_Autograd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#A Simple Example"
      ],
      "metadata": {
        "id": "ITodkANVGQ_k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwZVkcM3F1q5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.linspace(0., 2. * math.pi, steps=25,\n",
        "                   requires_grad=True)\n",
        "print(a)"
      ],
      "metadata": {
        "id": "KitjNuOZGYuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`requires_grad=True` means that in every computation that follows, autograd will be accumulating the history of the computation in the output tensors of that computation.\n",
        "\n",
        "Next, perform a computation and plot its output in terms of its inputs:"
      ],
      "metadata": {
        "id": "IqOF1qp_Gk0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "b = torch.sin(a)\n",
        "plt.plot(a.detach(), b.detach())"
      ],
      "metadata": {
        "id": "gzuOGYWfGkFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we print `b`, we see an indicator that it is tracking its computation history:"
      ],
      "metadata": {
        "id": "TIIw5iIdG-CL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(b)"
      ],
      "metadata": {
        "id": "DXc1ikz9G3Xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This `grad_fn` shows that when we execute the backpropagation step and compute gradients, we will need to compute the derivative of sin(x) for all this tensor's inputs.\n",
        "\n",
        "Let's perform some more computation"
      ],
      "metadata": {
        "id": "hfHUKB6nHE6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c = 2 * b\n",
        "print(c)"
      ],
      "metadata": {
        "id": "3a02bEWeHDOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = c + 1\n",
        "print(d)"
      ],
      "metadata": {
        "id": "R-fYHkF1HXv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = d.sum()\n",
        "print(out)"
      ],
      "metadata": {
        "id": "dmy7USCbHZNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we call `.backward()` on a tensor with no arguments, it expects the calling tensor to contain only a single element, as is the case when computing a loss function."
      ],
      "metadata": {
        "id": "eJUE5fOWHoUY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each `grad_fn` stored with our tensors allows us to walk the computation all the way back to its inputs with its `next_functions` property."
      ],
      "metadata": {
        "id": "4Rut3fYsHxq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('d:')\n",
        "print(d.grad_fn)\n",
        "print(d.grad_fn.next_functions)\n",
        "print(d.grad_fn.next_functions[0][0].next_functions)\n",
        "print(d.grad_fn.next_functions[0][0].next_functions[0][0].next_functions)\n",
        "print(d.grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions)"
      ],
      "metadata": {
        "id": "mxc1-ijHHh2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('c:')\n",
        "print(c.grad_fn)"
      ],
      "metadata": {
        "id": "MhGU4hryIP64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('b:')\n",
        "print(b.grad_fn)"
      ],
      "metadata": {
        "id": "sr9kqIAOIU21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('a:')\n",
        "print(a.grad_fn)"
      ],
      "metadata": {
        "id": "BseHbWtLIXJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that `a.grad_fn` is reported as `None`, indicating that this was an input to the function with no history of its own."
      ],
      "metadata": {
        "id": "tehqMLGcIaPI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get the derivatives, we need to call the `backward()` method on the output, and check the input's `grad` property ot inspect the graidents:"
      ],
      "metadata": {
        "id": "nrPfXjk3Ii1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out.backward()\n",
        "print(a.grad)\n",
        "plt.plot(a.detach(), a.grad.detach())"
      ],
      "metadata": {
        "id": "zaK8IbAvIZDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Be aware that only leaf nodes of the computation have their gradients computed. If we try to print `c.grad`, we will get back `None`."
      ],
      "metadata": {
        "id": "0PEvd7EsKA8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(c.grad)"
      ],
      "metadata": {
        "id": "0dvKfe1xIvOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Autograd in Training\n",
        "See how autograd actually works after a single training batch."
      ],
      "metadata": {
        "id": "NMETIRvSKQ7r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, define some constants, model, and stand-ins for inputs and outputs:"
      ],
      "metadata": {
        "id": "CKkoaNY3ud_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 16\n",
        "DIM_IN = 1000\n",
        "HIDDEN_SIZE = 100\n",
        "DIM_OUT = 10"
      ],
      "metadata": {
        "id": "47A_wGBIKO0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TinyModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TinyModel, self).__init__()\n",
        "        self.layer1 = torch.nn.Linear(1000, 100)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.layer2 = torch.nn.Linear(100, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "uLSbCgqSuoDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "some_input = torch.randn(BATCH_SIZE, DIM_IN,\n",
        "                         requires_grad=False)\n",
        "ideal_output = torch.randn(BATCH_SIZE, DIM_OUT,\n",
        "                           requires_grad=False)\n",
        "\n",
        "model = TinyModel()"
      ],
      "metadata": {
        "id": "xfCtw3OivWF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Within a subclass of `torch.nn.Module`, it is assumed that we want to track gradients on the layers' weights for learning.\n",
        "\n",
        "To see the layers of the model, we can examine the values of the weights, and verify that no gradients have been computed yet:"
      ],
      "metadata": {
        "id": "hC3_P3TGvifB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.layer2.weight[0][:10]) # just a small slice"
      ],
      "metadata": {
        "id": "BvQjzJDFviGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.layer2.weight.grad)"
      ],
      "metadata": {
        "id": "f7aczPdjv7D2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a loss function, we just use the square of the Euclidean distance between `prediction` and the `ideal_output`. Use basic SGD for optimizer:"
      ],
      "metadata": {
        "id": "GkO_ZR8lwFrX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "prediction = model(some_input)\n",
        "\n",
        "loss = (ideal_output - prediction).pow(2).sum()\n",
        "print(loss)"
      ],
      "metadata": {
        "id": "EDBInIigv_yL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we call `loss.backward()` now,"
      ],
      "metadata": {
        "id": "0iuJ0wp-wbV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()\n",
        "print(model.layer2.weight[0][:10])"
      ],
      "metadata": {
        "id": "c4T3R9enwZOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.layer2.weight.grad[0][:10])"
      ],
      "metadata": {
        "id": "u9UC21z7whds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gradients have been computed for each learning weight, but the weights remain unchanged, because we have not run the optimizer yet."
      ],
      "metadata": {
        "id": "Z5bh-z_Twr0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer.step()\n",
        "print(model.layer2.weight[0][:10])\n",
        "# layer2 weights have changed"
      ],
      "metadata": {
        "id": "XZAt9XPcwn54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.layer2.weight.grad[0][:10])"
      ],
      "metadata": {
        "id": "eYQzFkCHxUES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After calling `optimizer.step()`, we need to call `optimizer.zero_grad()`, or else every time we run `loss.backward()`, the gradients on the learning weights will accumulate:"
      ],
      "metadata": {
        "id": "jLCatd6RyQtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.layer2.weight.grad[0][:10])\n",
        "\n",
        "for i in range(5):\n",
        "    prediction = model(some_input)\n",
        "    loss = (ideal_output - prediction).pow(2).sum()\n",
        "    loss.backward()\n",
        "print(model.layer2.weight.grad[0][:10])"
      ],
      "metadata": {
        "id": "JWLCRo6hxo_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer.zero_grad()\n",
        "print(model.layer2.weight.grad[0][:10])"
      ],
      "metadata": {
        "id": "MxVCBPAJzSD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Failing to zero the gradients before running our next training batch will cause the gradients to blow up in this manner, causing incorrect and unpredictable learning results."
      ],
      "metadata": {
        "id": "h9aBtCABzZwh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Turning Autograd off and on"
      ],
      "metadata": {
        "id": "bgegJlO1zjuo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The simplest way is to change the `requires_grad` flag on a tensor directly:"
      ],
      "metadata": {
        "id": "sfAr-bhpzpLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.ones(2, 3, requires_grad=True)\n",
        "print(a)"
      ],
      "metadata": {
        "id": "SAhuAOvnzV_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b1 = 2 * a\n",
        "print(b1)"
      ],
      "metadata": {
        "id": "_Bcc-sqVzzU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a.requires_grad = False\n",
        "b2 = 2 * a\n",
        "print(b2)"
      ],
      "metadata": {
        "id": "ZHK9cPDqz2Yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we only need autograd turned off temporarily, a better way is to use the `torch.no_grad()`:"
      ],
      "metadata": {
        "id": "DlXeKEJPz-y8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.ones(2,3, requires_grad=True) * 2\n",
        "b = torch.ones(2,3, requires_grad=True) * 3\n",
        "\n",
        "c1 = a + b\n",
        "print(c1)"
      ],
      "metadata": {
        "id": "yoELg5e5z7XU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    c2 = a + b\n",
        "print(c2)"
      ],
      "metadata": {
        "id": "FRsQjjYr0MtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c3 = a * b\n",
        "print(c3)"
      ],
      "metadata": {
        "id": "4adnCMuO0QXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`torch.no_grad()` can also be used as a function or method dectorator:"
      ],
      "metadata": {
        "id": "f8x7COr80UNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_tensor1(x, y):\n",
        "    return x + y\n",
        "\n",
        "@torch.no_grad()\n",
        "def add_tensor2(x, y):\n",
        "    return x + y\n",
        "\n",
        "a = torch.ones(2,3, requires_grad=True) * 2\n",
        "b = torch.ones(2,3, requires_grad=True) * 3"
      ],
      "metadata": {
        "id": "gI-6Eslq0SqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c1 = add_tensor1(a, b)\n",
        "print(c1)"
      ],
      "metadata": {
        "id": "U3b8EixE0kNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c2 = add_tensor2(a, b)\n",
        "print(c2)"
      ],
      "metadata": {
        "id": "j6Oltg1_0l6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a corresponding context manager, `torch.enable_grad()`, for turning autograd on when it is not already. It may also be used as a decorator.\n",
        "\n",
        "If we have a tensor that requires gradient tracking but we want a copy that does not, we can use the `Tensor` object's `detach()` method - it creates a copy of the tensor that is *detached* from the computation history:"
      ],
      "metadata": {
        "id": "ZL0lazEC0tgA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(5, requires_grad=True)\n",
        "y = x.detach()\n",
        "\n",
        "print(x)\n",
        "print(y)"
      ],
      "metadata": {
        "id": "LiD8Oa_50qz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Autograd and In-place Operations\n",
        "Autograd needs the intermediate values of a computation to perform gradient computations. For this reason, we must be careful about using in-place operations when using autograd."
      ],
      "metadata": {
        "id": "OmH1vGHJ47s4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.linspace(0., 2. * math.pi, steps=25,\n",
        "                   requires_grad=True)\n",
        "torch.sin_(a)"
      ],
      "metadata": {
        "id": "rPWp6VmL1odl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Autograd Profier\n",
        "The computation history combined with timing information will make a handy profiler - and autograd has that feature baked in:"
      ],
      "metadata": {
        "id": "JZ4I_1kV5Voh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cpu')\n",
        "run_on_gpu = False\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    run_on_gpu = True\n",
        "\n",
        "x = torch.rand(2,3, requires_grad=True)\n",
        "y = torch.rand(2,3, requires_grad=True)\n",
        "z = torch.ones(2,3, requires_grad=True)"
      ],
      "metadata": {
        "id": "dIyn6QdP5Ubd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.autograd.profiler.profile(use_cuda=run_on_gpu) as prf:\n",
        "    for _ in range(1000):\n",
        "        z = (z / x) * y\n",
        "\n",
        "print(prf.key_averages().table(sort_by='self_cpu_time_total'))"
      ],
      "metadata": {
        "id": "SRdxx2ci8xgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This profier can also label individual sub-blocks of code, break out the data by input tensor shape, and export data as a Chrome tracing tools file."
      ],
      "metadata": {
        "id": "5n9_uent9Ei9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#More Autograd Detail\n",
        "\n",
        "`torch.autograd` is an engine for computing gradients. The `backward()` call can also take an optional vector input. This vector represents a set of gradients over the tensor, which are multiplied by the Jacobian of the autograd-traced tensor that precedes it:"
      ],
      "metadata": {
        "id": "Uc3vYiHK9VYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(3, requires_grad=True)\n",
        "\n",
        "y = x * 2\n",
        "while y.data.norm() < 1000:\n",
        "    y = y * 2\n",
        "print(y)"
      ],
      "metadata": {
        "id": "J9I7TPdj8_5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we try to call `y.backward()`, we will get a runtime error:"
      ],
      "metadata": {
        "id": "VDlQFL_S9xbx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward()"
      ],
      "metadata": {
        "id": "bb2SGbVV9ub1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a multi-dimensional output, autograd expets us to provide gradients for those three outputs that it can multiply into the Jacobian:"
      ],
      "metadata": {
        "id": "jgehjAEu99S4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float) # stand-in for gradients\n",
        "y.backward(v)\n",
        "print(x.grad)"
      ],
      "metadata": {
        "id": "8NLM91w_94Hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The High-Level API\n",
        "There is an API on autograd that gives us direct access to important differential matirx and vector operations."
      ],
      "metadata": {
        "id": "wCwaAJsc-PQa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To calculate the Jacobian of a simple function, evaluated for a 2 single-element inputs:"
      ],
      "metadata": {
        "id": "A5BVZn_c-a8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def exp_adder(x,y):\n",
        "    return 2 * x.exp() + 3 * y\n",
        "\n",
        "inputs = (torch.rand(1), torch.rand(1)) # arguments for the function\n",
        "print('Inputs:')\n",
        "print(inputs)\n",
        "print('\\nJacobian:')\n",
        "torch.autograd.functional.jacobian(exp_adder, inputs)"
      ],
      "metadata": {
        "id": "q-nLeBID-MDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also do this with higher-order tensors:"
      ],
      "metadata": {
        "id": "ob1AV0sa-1rd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = (torch.rand(3), torch.rand(3))\n",
        "print('Inputs:')\n",
        "print(inputs)\n",
        "print('\\nJacobian:')\n",
        "torch.autograd.functional.jacobian(exp_adder, inputs)"
      ],
      "metadata": {
        "id": "vOVzQilG-qz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also compute the Hessian with `torch.autograd.functional.hessian()` method:"
      ],
      "metadata": {
        "id": "HPZO1_jnAAaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = (torch.rand(1), torch.rand(1))\n",
        "print('Inputs:')\n",
        "print(inputs)\n",
        "print('\\nHessian:')\n",
        "torch.autograd.functional.hessian(exp_adder, inputs)"
      ],
      "metadata": {
        "id": "Tvorehhv_FVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is also a function to directly compute the vector-Jacobian product:"
      ],
      "metadata": {
        "id": "94vQJKkUAhx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def do_some_doubling(x):\n",
        "    y = x * 2\n",
        "    while y.data.norm() < 1000:\n",
        "        y = y * 2\n",
        "    return y\n",
        "\n",
        "inputs = torch.randn(3)\n",
        "my_gradients = torch.tensor([0.1, 1., 0.0001])\n",
        "torch.autograd.functional.vjp(do_some_doubling, inputs, v=my_gradients)"
      ],
      "metadata": {
        "id": "DsK583BAAMu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `torch.autograd.functional.jvp()` method performs the same matrix multipilication as `vjp()` with the operands reversed. The `vhp()` and `hvp()` methods do the same for a vector-Hessian product."
      ],
      "metadata": {
        "id": "jmFj6cDTA6bj"
      }
    }
  ]
}